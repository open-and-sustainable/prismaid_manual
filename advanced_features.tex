\chapter[Ensemble Reviews]{Advanced Features: Ensemble Reviews} \label{chap:advanced_ensemble}

This chapter explores ensemble reviews, one of the most powerful advanced features of \texttt{prismAId}, enabling users to leverage multiple AI models simultaneously for enhanced reliability and insight.

\section{Understanding Ensemble Reviews}

Ensemble reviews represent a sophisticated approach to systematic literature analysis where multiple large language models (LLMs) are used to extract information from the same document set. This methodology parallels the traditional practice of having multiple human reviewers assess the same literature.

\subsection{Concept and Benefits}\tip{Ensemble reviews are particularly valuable when conducting high-stakes systematic reviews where accuracy and reliability are critical, such as in clinical guideline development or policy formation.}

\begin{itemize}
    \item \textbf{Definition}: An ensemble review utilizes two or more AI models, either from the same provider or across different providers, to independently extract information from each document.
    \item \textbf{Theoretical Foundation}: The approach is grounded in ensemble learning theory, where multiple algorithms collectively produce more accurate and robust results than individual algorithms.
\end{itemize}

\begin{configbox}[Configuration: Simple Ensemble with Two Models]
\begin{lstlisting}[language=TOML]
[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-4o-mini"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "OpenAI"
api_key = ""
model = "gpt-4o"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\subsection{Key Advantages}

Ensemble reviews offer several significant benefits:

\begin{itemize}
    \item \textbf{Uncertainty Quantification}: Variations in model outputs highlight where information may be ambiguous or open to interpretation.
    \item \textbf{Increased Confidence}: Strong agreement across models suggests higher reliability of the extracted information.
    \item \textbf{Bias Reduction}: Different models may have different biases; using multiple models helps identify and mitigate these biases.
    \item \textbf{Robustness to Model Limitations}: Different models excel at different tasks; using multiple models compensates for individual weaknesses.
\end{itemize}

\section{Configuring Multi-Model Ensembles}

\texttt{prismAId} offers exceptional flexibility in configuring ensemble reviews, allowing you to combine models from the same provider, different providers, or a mix of both.

\subsection{Using Multiple Models from a Single Provider}

This approach allows you to compare different models with varying capabilities from the same provider:\tip{When using multiple models from the same provider, consider including a range of model sizes to balance cost, speed, and accuracy. For example, include both gpt-3.5-turbo and gpt-4o to compare a faster, more economical model with a more sophisticated one.}

\begin{configbox}[Configuration: Ensemble with Multiple OpenAI Models]
\begin{lstlisting}[language=TOML]
[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-3.5-turbo"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "OpenAI"
api_key = ""
model = "gpt-4o-mini"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.3]
provider = "OpenAI"
api_key = ""
model = "gpt-4o"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\subsection{Cross-Provider Ensemble Configuration}

Using models from different providers can be particularly valuable, as these models may have been trained on different datasets and using different architectures:\warning{When using models from multiple providers, ensure you have valid API keys for each provider configured either in your TOML file or as environment variables. Missing or invalid keys will cause the review to fail for that provider.}

\begin{configbox}[Configuration: Cross-Provider Ensemble Example]
\begin{lstlisting}[language=TOML]
[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-4o-mini"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "GoogleAI"
api_key = ""
model = "gemini-1.5-flash"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.3]
provider = "Anthropic"
api_key = ""
model = "claude-3-haiku"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\subsection{Comprehensive Five-Provider Ensemble}

For maximum diversity and robustness, you can configure an ensemble using models from all supported providers:\reminder{Pay close attention to the temperature settings in ensemble reviews. Lower temperatures (0.01-0.1) produce more deterministic outputs, making it easier to compare responses across models.}


\begin{configbox}[Configuration: Full Five-Provider Ensemble]
\begin{lstlisting}[language=TOML]
[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-4o-mini"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "GoogleAI"
api_key = ""
model = "gemini-1.5-flash"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.3]
provider = "Cohere"
api_key = ""
model = "command-r"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.4]
provider = "Anthropic"
api_key = ""
model = "claude-3-haiku"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.5]
provider = "DeepSeek"
api_key = ""
model = "deepseek-chat"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\section{Analyzing Ensemble Results}

After running an ensemble review, you'll need specialized approaches to analyze and interpret the results from multiple models.

\subsection{Output Format and Structure}

When running an ensemble review, \texttt{prismAId} creates separate result files for each model used:

\begin{infobox}[Example Output Files from Ensemble Review]
\begin{lstlisting}
results_OpenAI_gpt-4o-mini.csv
results_GoogleAI_gemini-1.5-flash.csv
results_Anthropic_claude-3-haiku.csv
\end{lstlisting}
\end{infobox}

The individual model files contain the standard extraction results. Analyzing them, researchers whould prepare ensemble summary file containing consensus metrics and comparison data.

\subsection{Quantifying Model Agreement}

To analyze the level of agreement between models:\tip{For categorical variables, Cohen's kappa or Fleiss' kappa (for more than two models) can provide a more sophisticated measure of inter-model agreement than simple percentage agreement.}

\begin{commandbox}[R Code for Analyzing Model Agreement]
\begin{lstlisting}[language=R]
library(tidyverse)

# Load results from different models
model1 <- read_csv("results_OpenAI_gpt-4o-mini.csv")
model2 <- read_csv("results_GoogleAI_gemini-1.5-flash.csv")
model3 <- read_csv("results_Anthropic_claude-3-haiku.csv")

# Join datasets
comparison <- model1 %>%
  select(filename, crop_type, climate_factor) %>%
  rename(crop_type_model1 = crop_type,
         climate_factor_model1 = climate_factor) %>%
  left_join(
    model2 %>%
      select(filename, crop_type, climate_factor) %>%
      rename(crop_type_model2 = crop_type,
             climate_factor_model2 = climate_factor),
    by = "filename"
  ) %>%
  left_join(
    model3 %>%
      select(filename, crop_type, climate_factor) %>%
      rename(crop_type_model3 = crop_type,
             climate_factor_model3 = climate_factor),
    by = "filename"
  )

# Calculate agreement for categorical variables
comparison <- comparison %>%
  mutate(
    crop_type_agreement = case_when(
      crop_type_model1 == crop_type_model2 & crop_type_model2 == crop_type_model3 ~ "full_agreement",
      crop_type_model1 == crop_type_model2 | crop_type_model1 == crop_type_model3 | crop_type_model2 == crop_type_model3 ~ "partial_agreement",
      TRUE ~ "no_agreement"
    ),
    climate_factor_agreement = case_when(
      climate_factor_model1 == climate_factor_model2 & climate_factor_model2 == climate_factor_model3 ~ "full_agreement",
      climate_factor_model1 == climate_factor_model2 | climate_factor_model1 == climate_factor_model3 | climate_factor_model2 == climate_factor_model3 ~ "partial_agreement",
      TRUE ~ "no_agreement"
    )
  )

# Summarize agreement levels
agreement_summary <- comparison %>%
  summarize(
    crop_type_full_agreement = mean(crop_type_agreement == "full_agreement"),
    crop_type_partial_agreement = mean(crop_type_agreement == "partial_agreement"),
    crop_type_no_agreement = mean(crop_type_agreement == "no_agreement"),
    climate_factor_full_agreement = mean(climate_factor_agreement == "full_agreement"),
    climate_factor_partial_agreement = mean(climate_factor_agreement == "partial_agreement"),
    climate_factor_no_agreement = mean(climate_factor_agreement == "no_agreement")
  )

print(agreement_summary)
\end{lstlisting}
\end{commandbox}


\subsection{Visualizing Ensemble Results}

Visualizations can help identify patterns of agreement and disagreement:\note{For numerical data extraction (like percentages or measurements), consider using scatterplots to visualize the correlation between values extracted by different models, or boxplots to show the distribution of values across models.}

\begin{commandbox}[Python Code for Visualizing Ensemble Agreement]
\begin{lstlisting}[language=Python]
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load results from different models
model1 = pd.read_csv("results_OpenAI_gpt-4o-mini.csv")
model2 = pd.read_csv("results_GoogleAI_gemini-1.5-flash.csv")
model3 = pd.read_csv("results_Anthropic_claude-3-haiku.csv")

# Create comparison dataframe for a specific variable
comparison = pd.DataFrame({
    'filename': model1['filename'],
    'OpenAI': model1['crop_type'],
    'GoogleAI': model2['crop_type'],
    'Anthropic': model3['crop_type']
})

# Count agreements for each paper
comparison['agreement_count'] = comparison.apply(
    lambda row: len(set([row['OpenAI'], row['GoogleAI'], row['Anthropic']])),
    axis=1
)

# Create a heatmap of disagreements
# Convert to wide format with models as columns and papers as rows
heatmap_data = comparison.set_index('filename')[['OpenAI', 'GoogleAI', 'Anthropic']]

# Create categorical mapping for visualization
unique_values = sorted(list(set(
    heatmap_data['OpenAI'].tolist() +
    heatmap_data['GoogleAI'].tolist() +
    heatmap_data['Anthropic'].tolist()
)))
value_map = {value: i for i, value in enumerate(unique_values)}

# Apply mapping
for col in heatmap_data.columns:
    heatmap_data[col] = heatmap_data[col].map(value_map)

# Create heatmap
plt.figure(figsize=(12, len(heatmap_data) * 0.3))
sns.heatmap(heatmap_data, cmap='viridis', yticklabels=True)
plt.title('Model Agreement Visualization (Crop Type)')
plt.tight_layout()
plt.savefig('ensemble_agreement_heatmap.png', dpi=300)

# Create agreement distribution chart
agreement_counts = comparison['agreement_count'].value_counts().sort_index()
plt.figure(figsize=(10, 6))
agreement_counts.plot(kind='bar')
plt.title('Distribution of Model Agreement')
plt.xlabel('Number of Unique Answers (1 = Full Agreement)')
plt.ylabel('Number of Papers')
plt.tight_layout()
plt.savefig('agreement_distribution.png', dpi=300)
\end{lstlisting}
\end{commandbox}


\subsection{Decision Strategies for Conflicting Results}

When models disagree on extracted information, several strategies can be employed:\warning{Be cautious when using majority voting for numerical values, as this may lead to misleading results. For numerical data, consider using the median or mean, and examine the standard deviation to identify high-variance cases.}

\begin{itemize}
    \item \textbf{Majority Voting}: Use the value that the majority of models agree on.
    \item \textbf{Weighted Voting}: Assign higher weight to more capable models (e.g., GPT-4 over GPT-3.5).
    \item \textbf{Conservative Approach}: For papers with significant disagreement, flag for manual review.
    \item \textbf{Model-Specific Trust}: For certain types of information, one model may consistently outperform others.
\end{itemize}

\begin{commandbox}[Python Code for Majority Voting]
\begin{lstlisting}[language=Python]
import pandas as pd
from collections import Counter

# Load results from different models
model1 = pd.read_csv("results_OpenAI_gpt-4o-mini.csv")
model2 = pd.read_csv("results_GoogleAI_gemini-1.5-flash.csv")
model3 = pd.read_csv("results_Anthropic_claude-3-haiku.csv")

# Prepare a consolidated results dataframe
consolidated = pd.DataFrame({'filename': model1['filename']})

# Apply majority voting for each extracted field
for field in ['crop_type', 'climate_factor', 'adaptation_strategies_discussed']:
    consolidated[field] = [
        Counter([m1, m2, m3]).most_common(1)[0][0]
        for m1, m2, m3 in zip(
            model1[field].fillna(''),
            model2[field].fillna(''),
            model3[field].fillna('')
        )
    ]

    # For fields where all models disagree, mark for review
    consolidated[f'{field}_review_needed'] = [
        len(set([m1, m2, m3])) == 3
        for m1, m2, m3 in zip(
            model1[field].fillna(''),
            model2[field].fillna(''),
            model3[field].fillna('')
        )
    ]

# For numerical fields, take the median
for field in ['yield_impact_percentage']:
    try:
        # Convert to numeric, errors='coerce' will convert non-numeric to NaN
        vals1 = pd.to_numeric(model1[field], errors='coerce')
        vals2 = pd.to_numeric(model2[field], errors='coerce')
        vals3 = pd.to_numeric(model3[field], errors='coerce')

        # Calculate median
        consolidated[field] = [
            pd.Series([v1, v2, v3]).median()
            for v1, v2, v3 in zip(vals1, vals2, vals3)
        ]

        # Calculate standard deviation to identify high variance
        consolidated[f'{field}_std'] = [
            pd.Series([v1, v2, v3]).std()
            for v1, v2, v3 in zip(vals1, vals2, vals3)
        ]

        # Flag for review if standard deviation is high
        consolidated[f'{field}_review_needed'] = consolidated[f'{field}_std'] > 2.0
    except:
        # Handle cases where the field might not exist or can't be processed
        pass

# Save consolidated results
consolidated.to_csv("results_consolidated.csv", index=False)
\end{lstlisting}
\end{commandbox}

\section{Case Studies in Ensemble Reviews}

To illustrate the practical application of ensemble reviews, let's examine some case scenarios and best practices.

\subsection{Case 1: Basic Verification Ensemble}

\begin{configbox}[Configuration: Verification Ensemble]
\begin{lstlisting}[language=TOML]
# A simple two-model ensemble using different providers
# for basic verification of extraction results

[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-3.5-turbo"  # More economical option
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "Anthropic"
api_key = ""
model = "claude-3-haiku"  # Similar capability level
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\textbf{Use Case}: This configuration is suitable for routine systematic reviews where you want a basic cross-check between providers without significantly increasing costs. It uses comparably efficient models from different providers.

\textbf{Expected Outcome}: When these models agree, you can have higher confidence in the extraction. When they disagree, you might flag those specific data points for manual review.

\subsection{Case 2: Tiered Capability Ensemble}

\begin{configbox}[Configuration: Tiered Capability Ensemble]
\begin{lstlisting}[language=TOML]
# A three-model ensemble using progressively more capable models
# from the same provider for comparative analysis

[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-3.5-turbo"  # Base level
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "OpenAI"
api_key = ""
model = "gpt-4o-mini"  # Mid level
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.3]
provider = "OpenAI"
api_key = ""
model = "gpt-4o"  # Top level
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\textbf{Use Case}: This approach is useful for understanding how model capability affects extraction quality. It can help determine if investing in more advanced models provides meaningful improvements for your specific extraction tasks.

\textbf{Expected Outcome}: By comparing results across models of increasing capability, you can determine the minimum model level needed for reliable extraction, potentially optimizing future costs.

\subsection{Case 3: Comprehensive Cross-Provider Ensemble}

\begin{configbox}[Configuration: Comprehensive Cross-Provider Ensemble]
\begin{lstlisting}[language=TOML]
# A five-provider ensemble using top-tier models from each provider
# for maximum reliability in high-stakes reviews

[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-4o"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.2]
provider = "GoogleAI"
api_key = ""
model = "gemini-1.5-pro"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.3]
provider = "Cohere"
api_key = ""
model = "command-r-plus"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.4]
provider = "Anthropic"
api_key = ""
model = "claude-3-opus"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0

[project.llm.5]
provider = "DeepSeek"
api_key = ""
model = "deepseek-chat"
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

\textbf{Use Case}: This configuration is ideal for high-stakes systematic reviews such as those informing clinical guidelines, policy decisions, or major meta-analyses where maximum reliability is essential.\tip{Start with a small test dataset when using comprehensive ensembles like this to estimate costs and review time before committing to your full dataset.}

\textbf{Expected Outcome}: This approach provides the highest confidence in results through diverse model architectures and training data. Points of unanimous agreement across all five providers suggest very high reliability.


\subsection{Best Practices for Ensemble Reviews}

Based on case studies and practical experience, here are recommended best practices:\reminder{When reporting results from ensemble reviews in publications, clearly state which models were used, how disagreements were resolved, and what level of agreement was observed for key findings.}

\begin{itemize}
    \item \textbf{Start Small}: Test your ensemble configuration on a small subset (5-10 papers) to identify any issues before running the full review.
    \item \textbf{Match Complexity to Need}: Use simpler ensembles for routine reviews and more comprehensive ensembles for high-stakes reviews.
    \item \textbf{Control Variables}: Keep temperature settings consistent across models to ensure fair comparisons.
    \item \textbf{Budget Appropriately}: Ensemble reviews will multiply your API costs by the number of models used; plan accordingly.
    \item \textbf{Document Model Versions}: Note the specific model versions used, as providers regularly update their models.
    \item \textbf{Analyze Disagreements}: Patterns in model disagreements often reveal ambiguities in your prompt or in the literature itself.
\end{itemize}

\chapter[Debugging, Costs \& Integration]{Advanced Features: Debugging, Cost Management \& Integration} \label{chap:advanced_features}

Building on our exploration of ensemble reviews, this chapter covers additional advanced features of \texttt{prismAId} that can enhance your systematic review process: debugging techniques, cost management strategies, advanced prompt engineering, and workflow integration.

\section{Debugging and Validation Techniques}

\texttt{prismAId} offers several advanced debugging features that can help identify and resolve issues in your systematic review process.

\subsection{Log Levels and Debugging Output}

The \texttt{log\_level} parameter in your configuration file controls the verbosity of debugging information:

\begin{configbox}[Configuration: Log Level Settings]
\begin{lstlisting}[language=TOML]
[project.configuration]
log_level = "high"  # Options: "low", "medium", "high"
\end{lstlisting}
\end{configbox}

Each level provides different information:\tip{When troubleshooting issues, start by setting the log level to "medium" to get more detailed console output. If the issue persists, switch to "high" for complete logging to file.}
\begin{itemize}
    \item \textbf{Low}: Minimal information, only essential status updates
    \item \textbf{Medium}: Detailed process information printed to the console
    \item \textbf{High}: Comprehensive logs saved to a file, including all API interactions
\end{itemize}

\begin{commandbox}[Command: Running with High Log Level]
\begin{lstlisting}[language=Bash]
# First set log_level to "high" in your config file
./prismaid -project your_project.toml

# Check the log file created in your project directory
cat your_project.log
\end{lstlisting}
\end{commandbox}

\subsection{Duplication for Consistency Testing}

The duplication feature runs the same review twice on identical inputs, allowing you to assess the consistency of model responses:\warning{Enabling duplication will double your API usage and costs. Use this feature selectively for testing or when consistency validation is critical.}

\begin{configbox}[Configuration: Enabling Duplication]
\begin{lstlisting}[language=TOML]
[project.configuration]
duplication = "yes"  # Options: "yes", "no"
\end{lstlisting}
\end{configbox}

The duplication process:
\begin{enumerate}
    \item Creates temporary copies of your input files
    \item Processes them as a separate batch
    \item Compares results for consistency
    \item Cleans up temporary files
\end{enumerate}

\subsection{Chain-of-Thought Justification}

The Chain-of-Thought (CoT) justification feature provides visibility into the model's reasoning process:

\begin{configbox}[Configuration: Enabling CoT Justification]
\begin{lstlisting}[language=TOML]
[project.configuration]
cot_justification = "yes"  # Options: "yes", "no"
\end{lstlisting}
\end{configbox}

When enabled, this creates a separate .txt file for each processed document containing:\tip{CoT justifications are invaluable for validating extraction quality, identifying potential misinterpretations, and understanding why models might be struggling with particular papers or information types.}
\begin{itemize}
    \item The model's reasoning for each extracted data point
    \item Key passages from the document that influenced the extraction
    \item Any uncertainty or alternative interpretations considered
\end{itemize}


\section{Cost and Rate Management}

\texttt{prismAId} provides several features to help manage costs and API rate limits.

\subsection{Token and Request Rate Limiting}
\note{Each provider has different rate limit structures. OpenAI limits by TPM (tokens per minute), Anthropic by RPM (requests per minute), and others may have tiered systems. Check the provider's documentation for current limits.}

\begin{configbox}[Configuration: Rate Limiting Settings]
\begin{lstlisting}[language=TOML]
[project.llm.1]
provider = "OpenAI"
api_key = ""
model = "gpt-4o-mini"
temperature = 0.01
tpm_limit = 100000  # Tokens per minute limit
rpm_limit = 60      # Requests per minute limit
\end{lstlisting}
\end{configbox}

These settings help you:
\begin{itemize}
    \item \textbf{Avoid Provider Rate Limiting}: Stay below provider-enforced limits\warning{These limits can change as providers update their services. Always check the most recent documentation from each provider for current rate limits.}
    \item \textbf{Balance Resource Usage}: Prevent spikes in API consumption
\end{itemize}

\begin{infobox}[Example Provider Rate Limits]
\begin{lstlisting}
Provider  | Model            | Default TPM  | Default RPM
----------|------------------|--------------|------------
OpenAI    | gpt-3.5-turbo    | 60,000       | 3,500
OpenAI    | gpt-4o           | 10,000       | 500
Anthropic | claude-3-haiku   | N/A          | 5,000
GoogleAI  | gemini-1.5-flash | 4,000,000    | N/A
Cohere    | command-r        | 100,000      | 1,000
\end{lstlisting}
\end{infobox}

Please note that not respecting rate limits may result in API errors and in not obtaining results for some manuscripts.

\subsection{Automatic Cost Minimization}

\texttt{prismAId} can automatically select the most cost-effective model by leaving the model field empty:\tip{Automatic model selection is particularly useful for heterogeneous document collections where some papers may be too large for smaller models.}

\begin{configbox}[Configuration: Cost Minimization]
\begin{lstlisting}[language=TOML]
[project.llm.1]
provider = "OpenAI"
api_key = ""
model = ""  # Empty value enables automatic model selection
temperature = 0.01
tpm_limit = 0
rpm_limit = 0
\end{lstlisting}
\end{configbox}

How automatic model selection works:
\begin{enumerate}
    \item \texttt{prismAId} calculates the token count for each document
    \item It determines which provider models can handle the document within token limits
    \item Among eligible models, it selects the most cost-efficient option
    \item Different documents may be processed by different models based on size
\end{enumerate}

\section{Advanced Prompt Engineering}

The quality of your systematic review results largely depends on how effectively you structure your prompts. Here are advanced strategies for optimizing extraction through prompt engineering.

\subsection{Optimizing Prompt Components}

Each component of the prompt structure serves a specific purpose and can be optimized:

\begin{configbox}[Configuration: Advanced Prompt Components]
\begin{lstlisting}[language=TOML]
[prompt]
persona = "You are an expert environmental scientist specializing in climate change impacts on agriculture, with experience conducting systematic reviews according to PRISMA guidelines."
task = "Analyze the scientific paper provided and extract specific information about climate change impacts on crop yields, methodology used, and adaptation strategies discussed."
expected_result = "You should output a JSON object containing values for each key specified in the review structure. Values must adhere exactly to the prescribed formats and vocabulary."
definitions = "'Crop yield' refers to harvestable production per unit of land area. 'Statistical significance' means p < 0.05 unless otherwise specified in the paper. 'Adaptation strategies' include any interventions meant to reduce negative climate impacts on agriculture."
example = "For instance, if the paper states 'wheat yields decreased by 5.2% (p < 0.01) per degree Celsius warming, with irrigation mitigating 40% of losses', you would extract: {\"crop_type\": \"wheat\", \"climate_factor\": \"temperature increase\", \"yield_impact\": \"-5.2% per Â°C\", \"statistical_significance\": \"yes\", \"adaptation_strategies_discussed\": \"yes\", \"adaptation_effectiveness\": \"40% loss reduction\"}"
failsafe = "If specific information is not clearly stated in the document, do not speculate or infer beyond reasonable scientific interpretation. Respond with an empty string for missing data. If the paper doesn't address the topic at all, use 'not addressed' for categorical fields."
\end{lstlisting}
\end{configbox}

Key strategies for each component:\note{Research suggests that more detailed and domain-specific prompts generally produce more accurate extractions, particularly for specialized scientific concepts.}

\begin{itemize}
    \item \textbf{Persona}: Include relevant expertise and methodological background to establish appropriate context.
    \item \textbf{Task}: Be specific about analytical depth and the nature of extraction required.
    \item \textbf{Expected Result}: Define the exact output format and emphasize adherence to specified value constraints.
    \item \textbf{Definitions}: Provide domain-specific terminology explanations, especially for potentially ambiguous concepts.
    \item \textbf{Example}: Show realistic examples that cover different data patterns and edge cases.
    \item \textbf{Failsafe}: Include clear guidelines for handling uncertainty, ambiguity, and missing information.
\end{itemize}


\subsection{Advanced Review Structure Patterns}

Beyond context setting above, you may implement more sophisticated patterns in your review structure:

\begin{configbox}[Configuration: Advanced Review Structure Patterns]
\begin{lstlisting}[language=TOML]
# Hierarchical extraction
[review.1]
key = "methodology_type"
values = ["experimental", "observational", "modeling", "review", "mixed", "other", ""]

[review.2]
key = "experimental_design"
values = ["randomized controlled trial", "non-randomized controlled trial", "before-after", "other", "not applicable", ""]

# Conditional extraction
[review.3]
key = "sample_size_reported"
values = ["yes", "no"]

[review.4]
key = "sample_size_value"
values = [""]

# Relational extraction
[review.5]
key = "crop_types_studied"
values = [""]

[review.6]
key = "primary_crop"
values = [""]

# Confidence assessment
[review.7]
key = "statistical_methods_quality"
values = ["high", "medium", "low", "not applicable", "cannot determine", ""]

\end{lstlisting}
\end{configbox}

Advanced patterns include:\tip{When using advanced patterns, ensure your prompt includes clear instructions on how these patterns relate to each other, particularly for conditional extractions.}

\begin{itemize}
    \item \textbf{Hierarchical Extraction}: Extract general categories first, then specific details based on those categories.
    \item \textbf{Conditional Extraction}: Use yes/no fields to establish presence, then extract specific values only if present.
    \item \textbf{Relational Extraction}: Capture relationships between different entities (e.g., primary crop among multiple crops studied).
    \item \textbf{Confidence Assessment}: Include quality assessments of methodological elements.
\end{itemize}


\subsection{Iterative Prompt Refinement}

The most effective prompts are developed through an iterative process:\warning{Changing prompts mid-review can introduce inconsistency. Once you begin your full review, avoid modifying the prompt unless absolutely necessary. If changes are required, consider re-processing previously analyzed papers.}

\begin{infobox}[Iterative Prompt Development Process]
\begin{lstlisting}
1. Start with a basic prompt and review structure
2. Test on 3-5 representative papers
3. Analyze results for:
   - Incorrect extractions
   - Missing information
   - Ambiguous responses
   - Inconsistent formatting
4. Identify patterns in errors or weaknesses
5. Refine prompt components and review structure
6. Test again on the same papers plus 2-3 new ones
7. Repeat until results match expectations
8. Document all prompt versions and their performance
\end{lstlisting}
\end{infobox}

For more complex extractions, consider dividing review items across multiple project configurations, allowing for more tailored context prompts with specific examples. While this approach can improve extraction accuracy, be aware that it increases costs as manuscripts must be processed multiple times.

\section{Workflow Integration and Automation}

\texttt{prismAId} can be integrated into broader research workflows and automated pipelines.\reminder{For very large reviews (hundreds or thousands of papers), consider implementing a database backend to store intermediate results and enable incremental processing.}

\subsection{Integration with Research Workflows}

\texttt{prismAId} can be integrated with broader research tools and workflows:

\begin{itemize}
    \item \textbf{Reference Management}: Integrate with Zotero, Mendeley, or EndNote for literature organization
    \item \textbf{Statistical Analysis}: Export results to R, SPSS, or specialized meta-analysis software
    \item \textbf{Collaborative Research}: Share configurations and results through version control systems
    \item \textbf{Publication Workflows}: Generate formatted tables and figures for manuscript inclusion
\end{itemize}

\section{Conclusion}

Advanced features of \texttt{prismAId} significantly enhance its capabilities beyond basic systematic reviews.\tip{Start with simpler configurations and gradually incorporate advanced features as you become more familiar with \texttt{prismAId}. Each feature adds power but also complexity, so build your expertise incrementally.} Ensemble reviews provide increased confidence and uncertainty quantification. Debugging features help identify and resolve issues in the extraction process. Cost management strategies optimize resource utilization, while advanced prompt engineering improves the quality and reliability of extractions. Finally, programmatic integration and automation enable seamless incorporation of \texttt{prismAId} into comprehensive research workflows.

By mastering these advanced features, researchers can conduct more sophisticated, reliable, and efficient systematic reviews that maintain the highest standards of methodological rigor while leveraging the power of artificial intelligence.
